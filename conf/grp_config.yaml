defaults:
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

hydra:
  sweeper:
    sampler:
      seed: 123
    direction: minimize
    study_name: grp-model-optimization
    storage: null
    n_trials: 40
    n_jobs: 2
    params:
      batch_size: choice(64, 128, 256)
      learning_rate: range(1e-5, 1e-3, log=true)
      n_embd: choice(128, 256, 512)
      dropout: choice(0.1, 0.2, 0.3)

# Model configuration
model:
  vocab_size: 1000  # Adjust based on your dataset
  embed_dim: 512
  num_heads: 8
  num_blocks: 6
  dropout: 0.1
  patch_size: 8

stoi: {}  # This will be populated in the code
itos: {}  # This will be populated in the code

action_dim: 7  # Adjust based on your robot's action space

# Vocabulary configuration
vocab:
  chars: "abcdefghijklmnopqrstuvwxyz0123456789 "  # Add any other characters you need

# Training configuration
training:
  batch_size: 64
  num_epochs: 100
  learning_rate: 1e-4
  eval_interval: 100
  max_iters: 10000
  save_interval: 1000

output_dir: './checkpoints'

# Data configuration
data:
  dataset: gberseth/mini-bridge-mini64pix
  image_shape: [64, 64, 3]
  block_size: 32
  vocab_size: 32
  n_patches: 8
  action_bins: 10
  action_dim: 6

# Device configuration
device: "cuda"

# Seed for reproducibility
seed: 42

# Logging and checkpointing
wandb:
  project: "grp-project"
  entity: "your_entity"  # Replace with your wandb entity name

checkpointing:
  save_dir: "checkpoints"
  save_interval: 1000

# Debug mode
debug: true
